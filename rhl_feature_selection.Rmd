---
title: "Wine Feature Selection"
author: "Ryan Longmuir"
date: "4/16/2020"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float: yes
---


```{r setup, include = FALSE} 

knitr::opts_chunk$set(warning = F, message = F)
options(scientific=T, digits = 3) 

```


```{r packages & functions , include = F}

# -----------------------------------------------------------------------------
# loadPkg 
# -----------------------------------------------------------------------------

loadPkg = function(pkg, character.only = FALSE) { 
  if (!character.only) { pkg <- as.character(substitute(pkg)) }
  pkg <- ifelse(!character.only, as.character(substitute(pkg)) , pkg)  
  if (!require(pkg,character.only=T, quietly =T)) { install.packages(substitute(pkg),dep=T); if(!require(pkg,character.only=T)) stop("Package not found") } 
}

# -----------------------------------------------------------------------------
# packages 
# -----------------------------------------------------------------------------

loadPkg("dplyr")
loadPkg("kableExtra")
loadPkg("corrplot")
loadPkg("leaps")
loadPkg("faraway")
loadPkg("caret")
loadPkg("car")
loadPkg("stringr")
loadPkg("plotmo")
loadPkg("glmnet")
loadPkg("RColorBrewer")

# ------------------------------------------------------------------------
# plotting functions
# ------------------------------------------------------------------------

```

### Research Question 

Recall that in the previous iteration of this project, Data Wine'ing leveraged wine reviews collected from the magazine WineEnthusiast to conduct inferential statistics and uncover insights about wine quality. Data Wine'ing aimed to measure the degree of influence location had on the quality of a wine. More specifically Data Wine'ing posed the question "Are wines grown in prominent wine producing countries (e.g., Italy and France) rated higher than those grown in California?" Exploratory data analysis, inferential statistics and simple regression analysis revealed that location had a significant but subtle effect on wine quality, often less than one tenth of a point. This result has motivated Data Wine'ing's decision to broaden it's research question to explore the predictive power of the other variable available in the dataset. Data Wine'ing wishes to answer the following question "What are the factors that influence wine quality? Does there exist a limited number of factors that consumers can use to reliably choose wines of high quality?" 

### Data Wrangling 

Data Wine'ing previously used the Wine Reviews dataset publish by Zach Thoutt on Kaggle. Although the Wine Reviews dataset provided the Data Wine'ing team with a wealth of clean and informative data to fuel it's analysis of wine quality, there remained challenges classifying wine variety and location. Data Wine'ing seized this opportunity to develop simple binary variables to denote whether a wine is red or white and whether a wine is produced domestically in the United States or abroad. Furthermore Data Wine'ing has taken advantage of the Google Maps API to create new variable measuring longitude, latitude and elevation. Lastly using the critic's twitter handle, Data Wine'ing has created a variable measuring the number of followers a given critic has. Note, the number of followers associated with each of the critics is recorded as of the today rather than at the time of the review. 

```{r glance over the dataset}

setwd("~/Desktop/Git/edwinbet")
raw_wine_reviews.df <- read.csv("data/wine_reviews.csv") 

# # counting missing values & get a glimpse of the data
# raw_wine_reviews.df %>% summarise_all(function(x){sum((is.na(x)|x == ""))/nrow(.)})
# raw_wine_reviews.df %>% glimpse()

```

### Correlational Analysis 

Below is a simple correlation analysis of the following variables 

- points - The number of points awarded to the wine by WineEnthusiast 
- price - The price of the wine at the time of the review 
- year - The year the wine was produced, parsed from the title 
- red -  Equal to 1 if the variety of wine is red and 0 otherwise 
- dom - Equal to 1 if the wine produced in the United States 
- fol - The number of Twitter followers of the critic who wrote the review
- lon - The longitude of the region where the wine was produced 
- lat - The latitude of the region where the wine was produced 
- el - The elevation of the region where the wine was produced

As one might have expected, the greatest correlation is between the dom and lat, lon and el. The correlation table is consistent with the results in the previous iteration of this project, price is has a moderate positive correlation and year has a weak negative correlation with points. 


```{r corrplot}

# -----------------------------------------------------------------------------
# regular correlation analysis 
# -----------------------------------------------------------------------------

wine_reviews.df <- raw_wine_reviews.df %>%
  mutate(
    dom = ifelse(country == "US", 1, 0),
    red = ifelse(color == "red", 1, 0)
  ) %>%
  select(-c("X.1", "X", "country", "description", "designation", "province",
      "region_1", "region_2", "taster_name", "taster_twitter_handle",
      "title", "variety", "winery", "color"
  )) %>%
  mutate_all(as.numeric) %>%
  filter_all(function(x) {
    !is.na(x)
  })

# corrplot
wine_reviews.df %>%
  select(points, price, year, red, dom, fol,
    el, lat, lon) %>%
  cor() %>% corrplot.mixed()

# critics 
wine_reviews.df %>%
  select(
    points, "Alexander.Peartree", "Anna.Lee.C..Iijima", 
    "Carrie.Dykes", "Christina.Pickard", "Fiona.Adams", "Jeff.Jenssen",
    "Jim.Gordon", "Joe.Czerwinski", "Kerin.O.Keefe", "Lauren.Buzzeo",
    "Matt.Kettmann", "Michael.Schachner", "Mike.DeSimone", "Paul.Gregutt",
    "Roger.Voss", "Sean.P..Sullivan", "Susan.Kostrzewa", "Virginie.Boone"
  ) %>%
  cor() %>%
  corrplot.mixed()


```

### Feature Selection 

In this section Data Wine'ing employs multivariate regression analysis to estimate the effect the variables discussed the previous section on points. Data Wine'ing conducted an exhaustive search of linear models on points using the function `regsubsets()` from the the leaps package, keeping the best two subsets of explanatory variables for each size. Data Wine'ing then evaluated each of the models using Mallows's Cp (Cp), Bayesian Information Criterion (BIC) and the Adjusted R Squared. We found that the model including variables price, dom, Anna.Lee.C..Iijima, Jim.Gordon, Matt.Kettmann, Michael.Schachner, Paul.Gregutt, Virginie.Boone minimized the Cp, BIC and maximized the Adjusted R Squared. The variance inflation factor (VIF) of the model suggested no multicolinearity between coefficients. Furthermore when trained and tested on the separate partitions of the data the full model performed similarly in the training (0.225) and test datasets (0.223). 

```{r feature selection (adjr2)}

# subset for now 
# wine_reviews.df <- wine_reviews.df %>% select(-contains("country"))

# generate tables 
regsub <- regsubsets(points ~ . , data = wine_reviews.df, method = "exhaustive", nbest = 2)
# summary(regsub)

# make plots 
plot(regsub, scale = "adjr2")
plot(regsub, scale = "bic") 
plot(regsub, scale = "Cp") 

# make more plots 
p.info <- subsets(regsub,statistic = "adjr2", legend = F, min.size = 3, main = "Adjusted R Squared")
p.info <- subsets(regsub,statistic = "bic", legend = F, min.size = 3, main = "BIC")
p.info <- subsets(regsub,statistic = "cp", legend = F, min.size = 3, main = "Cp")
abline(a = 1, b = 1, lty = 2)

# # return best model using each of the metrics
#  which.max(summary(regsub)$adjr2) 
#  which.min(summary(regsub)$bic)
#  which.min(summary(regsub)$cp)

# build model
adjr2.m <- lm(data = wine_reviews.df, formula = points ~ price + Anna.Lee.C..Iijima +
    Jim.Gordon + Matt.Kettmann + Michael.Schachner + Paul.Gregutt + Virginie.Boone + dom)

# # check for multicollinearity
# vif(adjr2.m)

# -----------------------------------------------------------------------------
# Test full model 
# -----------------------------------------------------------------------------

# Partition dataset
set.seed(2)
wine_reviews.df <- mutate(wine_reviews.df, id = row_number(points))
train.df <- wine_reviews.df %>% sample_frac(.75)
test.df  <- anti_join(wine_reviews.df, train.df , by = 'id')

# Estimate model
model.m <- lm(points ~ . -id, data = train.df)

# # Make predictions and compute the R2, RMSE and MAE
# predict.v <- model.m %>% predict(test.df)
# data.frame(
#   R2 = R2(predict.v, test.df$points),
#   RMSE = RMSE(predict.v, test.df$points),
#   MAE = MAE(predict.v, test.df$points)
# )
# 
# # clean up 
# rm(adjr2.m, p.info, regsub, model.m, test.df, train.df, predict.v)
```


### Lasso Regression 

Although the stepwise regression revealed what might have been meaningful relationships in the data, stepwise regression is often criticized for producing R-squared values that are badly biased to be high.Therefore, Data Wine'ing has chosen to supplement it's feature selection with regularization. Below we estimate a LASSO regression and perform parameter tuning on the data. We find the model that minimized the mean squared error has a $\lambda$ of 0.001 and only sets the parameter of Anne.Krebiehl.MW equal to 0. In contrast the model which gives the most regularized model such that error is within one standard error of the minimum has as $\lambda$ of 0.0599 and sets the parameters of Anne.Krebiehl.MW, Carrie.Dykes, Christina.Pickard, Fiona.Adams, Jeff.Jenssen, Joe.Czerwinski, Kerin.O.Keef, Lauren.Buzzeo, Mike.DeSimone, Roger.Voss, Sean.P..Sullivan, lon and el equal to 0. We find that both model preform similar, explaining roughly $24\%$ and $23\%$ of the variation in the data respectively.  

```{r}

set.seed(123)    # set seed for reproducibility

# center y, X will be standardized in the modelling function
y <- wine_reviews.df %>% select(points) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X <- wine_reviews.df %>% select(-points, -id) %>% as.matrix()

# 10 fold cross validation
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
lasso <- cv.glmnet(X, y, lambda = lambdas_to_try, standardize = TRUE, nfolds = 10)

# plot cross validation 
plot(lasso, axes = F)
axis(side = 1, at = -10:10)

# -----------------------------------------------------------------------------
# test 
# -----------------------------------------------------------------------------

# fit model that minimizes square residuals 
lambda <- lasso$lambda.min
model <- glmnet(X, y, lambda = lambda, standardize = TRUE)
y_hat <- predict(model, X)
ssr <- t(y - y_hat) %*% (y - y_hat)
rsq_lasso_min <- cor(y, y_hat)^2
# model$beta


lambda <- lasso$lambda.1se
model <- glmnet(X, y, lambda = lambda, standardize = TRUE)
y_hat <- predict(model, X)
ssr <- t(y - y_hat) %*% (y - y_hat)
rsq_lasso_1se <- cor(y, y_hat)^2
# model$beta

# ---------------------------------------------------------------------------
# plot 
# ---------------------------------------------------------------------------

# plot lasso 
res <- glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = FALSE)
plot_glmnet(res, xvar = "lambda", label = T, xlim= c(-7, 2), col = brewer.pal(5, "Dark2"))

# clean up 
rm(X, y, lambda, y_hat, ssr, modle, res)
```










