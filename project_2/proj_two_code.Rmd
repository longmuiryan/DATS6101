---
title: "Data Science 6101 Project 2"
author: "Data Wine'ing"
date: "4/16/2020"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float: yes
---


```{r packages & functions , include = F}

setwd("~/Desktop/Git/edwinbet")
source("project_2/proj_two_functions.R")
set.seed(080461)

# -----------------------------------------------------------------------------
# packages 
# -----------------------------------------------------------------------------

# ryan's section 
loadPkg("tidyverse")
loadPkg("kableExtra")
loadPkg("corrplot")
loadPkg("leaps")
loadPkg("faraway")
loadPkg("car")
loadPkg("caret")
loadPkg("glmnet")  # for ridge regression
loadPkg("psych")   # for function tr() to compute trace of a matrix
loadPkg("xtable")
loadPkg("plotmo")
loadPkg("glmnet")
loadPkg("RColorBrewer")

# michael's section 
loadPkg(regclass)
loadPkg(pROC)
loadPkg(ResourceSelection)
loadPkg(pscl)
loadPkg(stringi)

# rima's section 
loadPkg(reshape2)
loadPkg(factoextra)
loadPkg(gridExtra)

# bryan's sectoin 
loadPkg(FNN)
loadPkg(gmodels)

# ------------------------------------------------------------------------
# knitr settings 
# ------------------------------------------------------------------------

knitr::opts_chunk$set(warning = F, message = F)
options(scientific=T, digits = 3) 

```

### Introduction 

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec vitae dapibus nunc. Praesent sit amet mauris nec velit porta iaculis. Quisque ornare risus id porttitor fermentum. Mauris aliquet, urna at eleifend tristique, nulla augue mollis sem, non tincidunt tortor elit non felis. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Duis sed dui fermentum, aliquam odio ut, posuere lacus. Mauris ut erat metus. Etiam in scelerisque magna. Fusce neque ipsum, semper ut mi vel, placerat porttitor quam. Donec pellentesque massa nec dui tempor, sit amet tincidunt lorem placerat. Donec condimentum enim sed dignissim viverra. Suspendisse quis tortor lorem. Nunc feugiat bibendum augue, et placerat ante luctus eu. Nulla nunc odio, eleifend molestie metus sit amet, volutpat pulvinar nisi. Aliquam nisi elit, tincidunt sed pretium ac, tempus in diam.

Sed et ullamcorper urna. Fusce eu nisi et dolor pharetra ultricies. Donec vel volutpat nulla. Donec et porta dui, eu maximus leo. Donec euismod non turpis ac dignissim. Ut vitae purus lobortis, fringilla dolor sed, vehicula quam. Curabitur cursus efficitur metus, at interdum elit consectetur ut. Quisque porttitor aliquet commodo. Donec non elit in nisl luctus hendrerit ac et felis. Proin ac pulvinar urna. In commodo enim est, at maximus ex feugiat sit amet. In sed rutrum orci.


<!-----------------------------------------------------------------------------
Ryan 
------------------------------------------------------------------------------> 


### Research Question 

Recall that in the previous iteration of this project, Data Wine'ing leveraged wine reviews collected from the magazine WineEnthusiast to conduct inferential statistics and uncover insights about wine quality. Data Wine'ing aimed to measure the degree of influence location had on the quality of a wine. More specifically Data Wine'ing posed the question "Are wines grown in prominent wine producing countries (e.g., Italy and France) rated higher than those grown in California?" Exploratory data analysis, inferential statistics and simple regression analysis revealed that location had a significant but subtle effect on wine quality, often less than one tenth of a point. This result has motivated Data Wine'ing's decision to broaden it's research question to explore the predictive power of the other variable available in the dataset. Data Wine'ing wishes to answer the following question "What are the factors that influence wine quality? Does there exist a limited number of factors that consumers can use to reliably choose wines of high quality?" 

### Data 


Data Wine'ing previously used the Wine Reviews dataset publish by Zach Thoutt on Kaggle. Although the Wine Reviews dataset provided the Data Wine'ing team with a wealth of clean and informative data to fuel it's analysis of wine quality, there remained challenges classifying wine variety and location. Data Wine'ing seized this opportunity to develop simple binary variables to denote whether a wine is red or white and whether a wine is produced domestically in the United States or abroad. Furthermore Data Wine'ing has taken advantage of the Google Maps API to create new variable measuring longitude,comp_lonitude and elevation. Lastly using the critic's twitter handle, Data Wine'ing has created a variable measuring the number of followers a given critic has. Note, the number of followers associated with each of the critics is recorded as of the today rather than at the time of the review. 

Below is a list of the features included in the Kaggle Wine Reviews dataset

- country - The country that the wine is from
- description 
- designation - The vineyard within the winery where the grapes that made the wine are from
- points - The number of points WineEnthusiast rated the wine on a scale of 1-100
- price - The cost for a bottle of the wine
- province - The province or state that the wine is
- region_1 - The wine growing area in a province or state (i.e, Napa)
- region_2 - Sometimes there are more specific regions specified within a wine growing area (ie Rutherford inside the Napa Valley), but this value can sometimes be blank
- taster_name - The type of grapes used to make the wine (i.e, Pinot Noir)
- taster_twitter_handle - The winery that made the wine
- title - The title of the wine review 
- variety - The type of grapes used to make the wine (i.e, Pinot Noir)

Below is a list of the features created using the Wine Reviews dataset  

- year - The year the wine was produced, parsed from the title of the wine 
- red - Equal to 1 if the variety of wine is red and 0 otherwise 
- dom - Eqial to 1 if the wine is grown in the United States and 0 otherwise 
- lat - The latitude of location where the wine was grown 
- lon - The longitude of the location where the wine was grown 
- el  - The elavation of the location where the wine was grown
- fol - The number of followers ofa the critic who reviewed the wine

### Exploratory Data Analysis 

```{r exploratory data analysis}

# read in raw data 
raw_wine_reviews.df <- read.csv("../data/wine_reviews.csv")

# prepare data 
wine_reviews.df <- raw_wine_reviews.df %>%
  mutate(
    red = ifelse(color == "red", 1, 0),
    dom = ifelse(country == "US", 1, 0)
  ) %>%
    rename(
    fol = taster_following, name = taster_name, 
    lat = comp_lat, lon = comp_lon, el = comp_el 
  ) %>% 
   bind_cols(as_tibble(model.matrix(~name + 0, . ))) %>% 
  select(points, price, year, red, dom, fol, el, lat, lon, contains("name"),
    -name, -`nameAnne KrebiehlÂ MW`) %>% 
  mutate_all(as.numeric) %>%
  filter_all(function(x){!is.na(x)})

# xkable summary 
wine_reviews.df %>%
  select(points, price, year, red, dom, fol, el, lat, lon) %>%
  cor() %>% xkablesummary()

```

Below is a simple correlational analysis of the following variables. As one might have expected, the greatest correlation is between the dom and lat, lon and el. The correlation table is consistent with the results in the previous iteration of this project, price is has a moderate postive correlation and year has a weak negetive correlation with points. Further more we have also displayed a correlation plot comparting wine critics and points, where each name is a binary variable is equal to 1 if named critic reviewed the wine and 0 otherwise. 


```{r correlational analysis}

# corrplot of primary variables 
wine_reviews.df %>%
  select(points, price, year, red, dom, fol, el, lat, lon) %>%
  cor() %>% corrplot.mixed()

# critic plot 
wine_reviews.df %>% 
  select(points, contains("name")) %>% 
  cor() %>% corrplot.mixed()

```

### Feature Selection 

In this section Data Wine'ing employs mutivariate regression analysis to estimate the effect the variables discussed the previous section on points. Data Wine'ing conducted an exhaustive search of linear models on points usings the function `regsubsets()` from the the leaps package, keeping the best two subsets of explanatory variables for each size. Data Wine'ing then evaluated each of the models using Mallows's Cp (Cp), Bayesian Information Criterion (BIC) and the Adjusted R Squared. We found that the fill model minimized the Cp, BIC and Adjusted R Squared. The variance inflation factor (VIF) of the full model suggested no multicolearity betwen coefficients. Furthermore when trained and tested on the seperate paritions of the data the full model peroformed similarly in the training (0.20) and test datasets (0.18). 

```{r feature selection analysis, echo = F}

# -----------------------------------------------------------------------------
# Exhaustive model search 
# -----------------------------------------------------------------------------

# generate tables 
wine_reviews.df <- wine_reviews.df %>% 
  select(points, price, year, red, dom, fol, el, lat, lon, "nameMatt Kettmann", "nameMichael Schachner")
regsub <- regsubsets(points ~ . , data = wine_reviews.df, method = "exhaustive", nbest = 2)
# summary(regsub)

# Make plots 
plot(regsub, scale = "adjr2", main = "Adjusted R Squared")
# plot(regsub, scale = "bic", main = "BIC")
# plot(regsub, scale = "Cp", main = "Cp")

# Make more plots 
p.info <- subsets(regsub, statistic = "adjr2", legend = F, min.size = 3, main = "Adjusted R Squared")
# p.info <- subsets(regsub,statistic = "bic", legend = F, min.size = 3, main = "BIC")
# p.info <- subsets(regsub,statistic = "cp", legend = F, min.size = 3, main = "Cp")
# abline(a = 1, b = 1, lty = 2)

# Return best model using each of the metrics
# which.max(summary(regsub)$adjr2)
# which.min(summary(regsub)$bic)
# which.min(summary(regsub)$cp)

# Build model
adjr2.m <- lm(data = wine_reviews.df, formula = points ~ .)
# summary(adjr2.m)

# Check for multicollinearity
# vif(adjr2.m)

# -----------------------------------------------------------------------------
# Test full model 
# -----------------------------------------------------------------------------

# Partition dataset
wine_reviews.df <- mutate(wine_reviews.df, id = row_number(points))
train.df <- wine_reviews.df %>% sample_frac(.75)
test.df  <- anti_join(wine_reviews.df, train.df , by = 'id')

# Estimate model
model.m <- lm(points ~ . -id, data = train.df)

# Make predictions and compute the R2, RMSE and MAE
# predict.v <- model.m %>% predict(test.df)
# data.frame(
#   R2 = R2(predict.v, test.df$points),
#   RMSE = RMSE(predict.v, test.df$points),
#   MAE = MAE(predict.v, test.df$points)
# )

# Output regression table 
wine_reviews.df %>% 
  lm(formula = points ~ . -id -el -lon) %>% 
  xtable() %>% 
  kable(digits = 5) %>%
  kable_styling(bootstrap_options = "striped", full_width = F,
  position = "left")

```


### Lasso Regression 

Although the stepwise regression revealed meaningful insights about the data, stepwise regression is often criticized for producing R-squared values that are badly biased to be high.Therefore, Data Wine'ing has chosen to supplement it's feature selection analysis with regularization. Below we estimate a LASSO regression and perform parameter tuning on the data. We find the model that minimized the mean squared error has a $\lambda$ of 0.001 and only sets the parameter of Anne.Krebiehl.MW equal to 0. In contrast the model which gives the most regularized model such that error is within one standard error of the minimum has as $\lambda$ of 0.0599 and sets the parameters of Anne.Krebiehl.MW, Carrie.Dykes, Christina.Pickard, Fiona.Adams, Jeff.Jenssen, Joe.Czerwinski, Kerin.O.Keef, Lauren.Buzzeo, Mike.DeSimone, Roger.Voss, Sean.P..Sullivan, lon and el equal to 0. We find that both model preform similar, explaining roughly $24\%$ and $23\%$ of the variation in the data respectively.  

```{r}

# center y, X will be standardized in the modelling function
y <- wine_reviews.df %>% select(points) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X <- wine_reviews.df %>% select(-points, -id) %>% as.matrix()

# 10 fold cross validation
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
lasso <- cv.glmnet(X, y, lambda = lambdas_to_try, standardize = TRUE, nfolds = 10)

# plot cross validation 
plot(lasso, axes = F)
axis(side = 1, at = -10:10)

# -----------------------------------------------------------------------------
# test 
# -----------------------------------------------------------------------------

# fit model that minimizes square residuals 
lambda <- lasso$lambda.min
model <- glmnet(X, y, lambda = lambda, standardize = TRUE)
y_hat <- predict(model, X)
ssr <- t(y - y_hat) %*% (y - y_hat)
rsq_lasso_min <- cor(y, y_hat)^2
# model$beta

lambda <- lasso$lambda.1se
model <- glmnet(X, y, lambda = lambda, standardize = TRUE)
y_hat <- predict(model, X)
ssr <- t(y - y_hat) %*% (y - y_hat)
rsq_lasso_1se <- cor(y, y_hat)^2
# model$beta

# ---------------------------------------------------------------------------
# plot 
# ---------------------------------------------------------------------------

res <- glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = FALSE)
plot_glmnet(res, xvar = "lambda", label = T, xlim= c(-7, 2), col = brewer.pal(5, "Dark2"))

rm(X, y, lambda, y_hat, ssr, model, res)
```





<!-----------------------------------------------------------------------------
Michael  
------------------------------------------------------------------------------> 

### Logistic Regressions 

```{r logit 1}

# Read in data 
wine_reviews.df <-  raw_wine_reviews.df %>%
  mutate(
    red = as.factor(ifelse(color == "red", 1, 0)),
    dom = as.factor(ifelse(country == "US", 1, 0)),
    p88 = ifelse(points > 88, 1, 0)
  ) %>%
  dplyr::select(
    points, price, year, red, dom, taster_following, comp_el, comp_lat, comp_lon, p88, taster_name
  ) %>% 
  filter_all(function(x){!is.na(x)})

# Estimate model 
glm <- glm(formula = p88 ~ . -points, data = wine_reviews.df, family = binomial(link = "logit"))
# exp(coef(glm)) 
xkabledply(glm)

```
Recall that we are interested in informing wine consumers what the best predictors are for selecting a quality wine, whether they're buying at their local wine store, or tasting wine at their favorite farmers market, we'd like to determine a metric for what really does constitute a "good" wine. In this section we aim to construct a logistic model that is capable of predicting whether a bottle of wine is above 88 points. 
The output variable of this logistic model will be `p88`, which is eqaul to 1 if a bottle has as score of above 88 points and above and 0 otherwise. An 88 point cutoff was selected as it nearly slices the dataset in half and makes for good use in model training and testing, including the fact that this score is a toes the line between the delineation for what is "very good" and "excellent" wines (and also is just above the average score for all wine enthusiast reviews https://www.wine-searcher.com/critics-17-wine+enthusiast). The model `glm(formula = p88 ~ . -points, data = wine_reviews.df, family = binomial(link="logit"))` was run to estimate `p88` and output can be found in the table below. 

We observe significant coefficients for all predictor variables, with the exception of `comp_lat`, latitude at which the reviewed wine was grown, and wine reviewers `taster_nameChristina Pickard` and `taster_nameVirginie Boone`. Notably, however, `taster_following`, `comp_el`, `comp_lat`, `comp_lon` provide no or a neglible effect on the odds-ratio for `p88` i.e., when points increase by one unit, these variables have no effect on the odds-ratio. This is interesting given the long-studied effects of geography on wine production - one may expect that growing wine in a place better-suited for growing grapes would produce a wine that is rated more highly than others, but our model suggests location has no effect on how point value assigned to the wine reviewed. This begs the question of what effect a particular reviewer has on point assignment. As we can see from the results in the model, and the odds-ratio coefficients, the levels `tasterName` i.e., those who reviewed the wine, play a signficiant role in predicting if a wine will be scored 89 points or better. More research is required to come a granlar analysis on the what exactly may influence the bias an individual reviewer has one a wine. 

The confusion matrix for the model in **table #** shows that the model was run with 73.5% accurarcy; the ROC-AUC curve accompanying this model returns a value 0.82, suggesting the model performs well when distinguishing between wines scored 89 or better; however, the result Hosmer-Lemeshow Goodness of Fit Test for the model suggests that there exists a significant difference between our model the observed data (p < .05). Further research and evaluation is required to build a model that can pass this test.

```{r model eval}
# Accuracy 
cm <- regclass::confusion_matrix(glm)
accuracy_glm <- (cm[1,1]+cm[2,2])/(cm[1,1] + cm[1,2] + cm[2,1] + cm[2,2])*100
cm %>% kable() %>% kable_styling()

# Hoslem text 
# ResourceSelection::hoslem.test(wine_reviews.df$p88, fitted(glm))

# Psuedo R^2
# pscl::pR2(glm)

# ROC-AUC
wine_reviews.df$prob <- predict(glm, type=c("response"))
roc <- pROC::roc(p88 ~ prob, data=wine_reviews.df)
# roc # ROC-AUC value = 0.82
plot(roc) 
```

```{r train and test}

# Scale dataset 
wine_reviews_z <- uzscale(wine_reviews.df, append=0, excl=c("p88"))

# Sample and subset 
wine_reviews_sample <- sample(2, nrow(wine_reviews_z), replace = TRUE, prob = c(0.75, 0.25))
wine_reviews_train <- filter(wine_reviews_z, wine_reviews_sample == 1) 
wine_reviews_test <- filter(wine_reviews_z, wine_reviews_sample == 2)

# Estimate logistic model using training data 
wine_reviews_logit <- glm(p88 ~ . -points, data = wine_reviews_train, family = binomial)
# summary(wine_reviews_logit)

wine_reviews_logit$xlevels[["taster_name"]] <- union(wine_reviews_logit$xlevels[["taster_name"]], levels(wine_reviews_test$taster_name))

# Use cutoff rule to classify type 
wine_reviews_test <- wine_reviews_test %>%
  mutate(
    p88_logit_p = predict(wine_reviews_logit, wine_reviews_test, type = c("response")),
    p88_logit = ifelse(p88_logit_p > 0.5, 1, 0)
  )

# Calculate accuracy
t <- table(wine_reviews_test$p88, wine_reviews_test$p88_logit)
# (t[1,1] + t[2,2]) / nrow(wine_reviews_test)

# Clean up
rm(wine_reviews_z, wine_reviews_sample, wine_reviews_train, wine_reviews_test, wine_reviews_logit, t)
```

<!-----------------------------------------------------------------------------
Bryan
------------------------------------------------------------------------------> 

### K Nearst Neighbor 

K-Nearest Neighbors (kNN) is one of the simplest of the clustering algorithms.  A supervised technique, kNN is a non-parametric, instance-based learning algorithm that utilizes the whole dataset as the model. For classifying new data, to predict a label for an input data point, kNN examines the close or 'nearest' neighbors of the input data in the space of feature vectors then outputs the label that appeared most often in that space. Here, nine features from the WineEnthusiast Wine Reviews dataset were chosen for analysis to predict a tenth feature, Points (p88). Of the nine features chosen, taster_name was not numeric and therefore not appropriate for kNN analysis. The final model used eight features, or variables from approximately 24,000 obsersations, with k-values ranging from k=3 to k=15. The best overall model was generated using k=13. Using k=15 showed a slighty better recall rate, but a lower F1 score, and the difference did not justify the increased computational expense. The model was shown to be approximately 75% accurate at predicting wine from the dataset with a score greater-than, less-than, or equal to 88 points. The data was well balanced at a near 50/50 spilt, indicating our model was a significant improvement over the null model.


```{r knn}


# -----------------------------------------------------------------------------
# Prepare data 
# -----------------------------------------------------------------------------

# wrangle data 
wine2 <- raw_wine_reviews.df %>%
  dplyr::mutate(
    red = ifelse(color == "red", 1, 0),
    dom = ifelse(country == "US", 1, 0),
    p88 = ifelse(points > 88, 1, 0)
  ) %>%
  dplyr::select(
    p88, price, year, red, dom, taster_following, taster_name,
    comp_lon, comp_lat, comp_el, p88
  ) %>%
  filter_all(function(x) !is.na(x))

# -----------------------------------------------------------------------------
# Create partition 
# -----------------------------------------------------------------------------

wine2 <- wine2 %>% dplyr::select(-p88, everything())
wine2 <- wine2 %>% dplyr::select(-taster_name, everything())
scaled_wine2 <- as.data.frame(scale(wine2[1:8], center = TRUE, scale = TRUE))

wine2_sample <- sample(2, nrow(scaled_wine2), replace=TRUE, prob=c(0.75, 0.25))

# subset 
wine2_training <- scaled_wine2[wine2_sample==1, 1:8]
wine2_test <- scaled_wine2[wine2_sample==2, 1:8]

# labels 
wine2.trainLabels <- wine2[wine2_sample==1, 9]
wine2.testLabels <- wine2[wine2_sample==2, 9]

# -----------------------------------------------------------------------------
# Find best K 
# -----------------------------------------------------------------------------

# create an empty dataframe to store the results from confusion matrices
cmdf = data.frame()

for (kval in 3:15) {
  # predict 
  wine2_pred <- knn(train = wine2_training, test = wine2_test, cl = wine2.trainLabels, k = kval)
  cm <- confusionMatrix(wine2_pred, reference = factor(wine2.testLabels ))
  # construct table 
  accuracy <- cm$overall['Accuracy']
  cmt <- data.frame(K = kval, Accuracy = accuracy, row.names = NULL ) # initialize a row of the metrics 
  cmt <- cbind(cmt, data.frame(t(cm$byClass))) # the dataframe of the transpose, with k valued added in front
  cmdf = rbind(cmdf, cmt)
}

# print table 
cmdf %>% 
  dplyr::select(K, Accuracy, Precision, Recall, Sensitivity) %>% 
  xkabledply()

```



<!-----------------------------------------------------------------------------
Reema 
------------------------------------------------------------------------------> 

### K Means 

Clustering techniques are crucial to data mining, especially when working with big data, due to their ability to create structure in a dataset, allowing the research to draw prelimnary insights. In other words, to narrow down a dataset to a managable size, clustering allows us to group observations that are alike. K-means is commonly used for splitting a dataset into a set number of groups. In the wine dataset, we are interested in understanding in identifying variables that could be strong predictors of the quality points a reviewer will score the wine. Our analysis indicated that the optimal number of groups the data could be divided into is five. Upon running a k-means, where we set k=5, we learned that the model believes that the 'Wine Taster Twitter Follower Numbers' is a strong predictor of how well the wine will be scored. For example, twitter users that have around a million twitter followers have almost exclusively reviewed wines after 2000 with an average price of 31.8 USD. Additionally, thought the latitude and longitude graphs we learned that cluster one has only reviewed wines from the coordinates 46.6, -119 which happens to fall on the west coast of the United States. By aggregating all this information, we are able to parse out a demographic of the wine reviewers.


```{r kmeans}

# -----------------------------------------------------------------------------
# Prepare data 
# -----------------------------------------------------------------------------

winedata <- raw_wine_reviews.df
wine <- winedata %>% 
 dplyr::mutate(
    red = ifelse(color == "red", 1, 0),
    dom = ifelse(country == "US", 1, 0)
  ) %>%
 dplyr::select(points, price, year, taster_following, comp_el, comp_lat, comp_lon) %>% 
 mutate_all(as.numeric) %>% 
 filter_all(function(x){!is.na(x)}) 

# Remove Outliers
outliers <- boxplot(wine$price, plot=FALSE)$out
wine2<- wine[-which(wine$price %in% outliers),]

# -----------------------------------------------------------------------------
# K Means 
#   Analysis inspired by https://uc-r.github.io/kmeans_clustering 
# ---------------------------------------------------------------------------

# function to compute total within-cluster sum of square
wss <- function(k) {
  kmeans(wine2, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)


plot(k.values, wss_values,
  type = "b", pch = 19, frame = FALSE,
  xlab = "Number of clusters K",
  ylab = "Total within-clusters sum of squares"
)

# n start is just the number of configurations, we dont really need that
k <- kmeans(wine2, centers = 5)
# print(k)

# Plots to compare
fviz_cluster(k, geom = "point",  data = wine2) + ggtitle("k = 5")

# Cluster Mean
# k$centers
# Number of values in each cluster
# k$size

# ---------------------------------------------------------------------------
# Summary statistics of clusters 
# ---------------------------------------------------------------------------

# mean
wine2 %>%
  mutate(Cluster = k$cluster) %>%
  group_by(Cluster) %>%
  summarize_all("mean") %>% 
  kable() %>% kable_styling()

# median
wine2 %>%
  mutate(Cluster = k$cluster) %>%
  group_by(Cluster) %>%
  summarize_all("median") %>% 
  kable() %>% kable_styling()

# ----------------------------------------------------------------------------
# Cluster box plots 
# ----------------------------------------------------------------------------

a <- clusterPlot(wine2, points, "Points")
b <- clusterPlot(wine2, price, "Price")
c <- clusterPlot(wine2, taster_following, "Taster Following")
d <- clusterPlot(wine2, comp_lon, "Longitude")
e <- clusterPlot(wine2, comp_lat, "Latitude")
f <- clusterPlot(wine2, comp_el, "Elavation")

grid.arrange(a, b, ncol = 2)
grid.arrange(c, d, ncol = 2)
grid.arrange(e, f, ncol = 2)


```













