---
title: "Data Science 6101 Project 2"
author: "Data Wine'ing"
date: "4/16/2020"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float: yes
    fig_width: 8
    fig_height: 4 
---


```{r packages & functions , include = F}

setwd("~/Desktop/Git/edwinbet")
source("project_2/proj_two_functions.R")
set.seed(080461)

# -----------------------------------------------------------------------------
# packages 
# -----------------------------------------------------------------------------

# ryan's section 
loadPkg("tidyverse")
loadPkg("kableExtra")
loadPkg("corrplot")
loadPkg("leaps")
loadPkg("faraway")
loadPkg("car")
loadPkg("caret")
loadPkg("glmnet")  # for ridge regression
loadPkg("psych")   # for function tr() to compute trace of a matrix
loadPkg("xtable")
loadPkg("plotmo")
loadPkg("glmnet")
loadPkg("RColorBrewer")

# michael's section 
loadPkg(regclass)
loadPkg(pROC)
loadPkg(ResourceSelection)
loadPkg(pscl)
loadPkg(stringi)

# rima's section 
loadPkg(reshape2)
loadPkg(factoextra)
loadPkg(gridExtra)

# bryan's sectoin 
loadPkg(FNN)
loadPkg(gmodels)

# ------------------------------------------------------------------------
# knitr settings 
# ------------------------------------------------------------------------

knitr::opts_chunk$set(warning = F, message = F)
options(scientific=T, digits = 3) 

```

### Introduction 

Point values assigned to a wine are well-known to separate which wines break through the limelight. However, oenophiles subjectively and arbitrarily quantify the points a wine receives - or how “good” a wine is. The present study extends the findings from our recent paper where we utilized the Wine Reviews dataset (Thoutt, 2017) and sought to compare the quality of New World versus Old World wine based on points and price (Irish, Longmuir, Domadia, & Pagan, 2020) and now seek to determine which factors influence wine quality. If there are a number of limited and reliable factors for the consumer to choose a high quality wine, then consumers should be able to quickly determine how “good” a wine is just from its label. 

For the purpose of this study wines that were scored 89 points or greater were considered high quality wines; wines below 89 points were low quality. New variables were introduced to the dataset to investigate the effect of other important factors when reviewing wine, such as the potential bias of a reviewer, the latitude, longitude, and elevation of where the grapes are grown, and the wine’s color. We leveraged the power of modeling techniques, such as stepwise regression, lasso regression, logistic regression, kNN, and K-means to discern which factors have the greatest influence on high quality wines. Our results are consistent with our previous project: year and price have a statistically significant effect on points. Furthermore, we were able to identify bias in the critics from the sample and, interestingly, found little to no effect of geography on a wine’s point assignment. Our classification modeling techniques were able to correctly classify high quality wines approximately 75% of the time and we were able to identify common characteristics among the clusters in the dataset.

<!-----------------------------------------------------------------------------
Ryan 
------------------------------------------------------------------------------> 


### Research Question 

Recall that in the previous iteration of this project, we leveraged wine reviews collected from the magazine WineEnthusiast to conduct inferential statistics and uncover insights about wine quality. We aimed to measure the degree of influence location had on the quality of a wine. More specifically, we posed the question, "Are wines grown in prominent wine-producing countries (e.g., Italy and France) rated higher than those grown in California?" Exploratory data analysis, inferential statistics, and simple regression analysis revealed that location had a significant but subtle effect on wine quality, often less than one-tenth of a point. This result motivated our decision to broaden our research question to explore the predictive power of the other variables available in the dataset. We would like to answer the following question: "What are the factors that influence wine quality and does there exist a limited number of factors that consumers can use to reliably choose wines of high quality?" 

### Data 

We previously used the Wine Reviews dataset published by Zach Thoutt on Kaggle. Although the Wine Reviews dataset provided us with a wealth of clean and informative data to fuel its analysis of wine quality, there remained challenges classifying wine variety and location. We seized this opportunity to develop simple binary variables to denote whether a wine is red or white and whether a wine is produced in the United States or abroad. Furthermore, the Google Maps API was used to create new variables measuring longitude, latitude, and elevation. Lastly a variable measuring the number of followers a given critic has was manually made using the critic’s twitter handle. Note, the number of followers associated with each of the critics is recorded as of April 20th, 2020 than at the time of the review. Table 1 diplays summary statistics points, price, year, red, dom, fol, el, lat, and lon. 

Below is a list of the features included in the Kaggle Wine Reviews dataset

* `country` - The country that the wine is from
description 
* `designation` - The vineyard within the winery where the grapes that made the wine are from
* `points` - The number of points WineEnthusiast rated the wine on a scale of 1-100
* `price` - The cost for a bottle of the wine
* `province` - The province or state that the wine is
* `region_1` - The wine growing area in a province or state (i.e., Napa)
* `region_2` - Sometimes there are more specific regions specified within a wine growing area (i.e., Rutherford inside the Napa Valley), but this value can sometimes be blank
* `name` - The name of the wine reviewer.
* `title` - The title of the wine review 
* `variety` - The type of grapes used to make the wine (i.e., Pinot Noir)

Below is a list of the features created using the Wine Reviews dataset  

* `year` - The year the wine was produced, parsed from the title of the wine 
* `red` - Equal to 1 if the variety of wine is red and 0 otherwise 
* `dom` - Equal to 1 if the wine is grown in the United States and 0 otherwise 
* `lat` - The latitude of location where the wine was grown 
* `lon` - The longitude of the location where the wine was grown 
* `el`  - The elevation of the location where the wine was grown
* `fol` - The number of followers of a the critic who reviewed the wine


### Exploratory Data Analysis 

```{r exploratory data analysis}

# read in raw data 
raw_wine_reviews.df <- read.csv("../data/wine_reviews.csv")

# prepare data 
wine_reviews.df <- raw_wine_reviews.df %>%
  mutate(
    red = ifelse(color == "red", 1, 0),
    dom = ifelse(country == "US", 1, 0)
  ) %>%
    rename(
    fol = taster_following, name = taster_name, 
    lat = comp_lat, lon = comp_lon, el = comp_el 
  ) %>% 
   bind_cols(as_tibble(model.matrix(~name + 0, . ))) %>% 
  select(points, price, year, red, dom, fol, el, lat, lon, contains("name"),
    -name, -`nameAnne Krebiehl MW`, -name1) %>% 
  mutate_all(as.numeric) %>%
  filter_all(function(x){!is.na(x)})

# xkable summary 
wine_reviews.df %>%
  select(points, price, year, red, dom, fol, el, lat, lon) %>%
  cor() %>% xkablesummary(title = "Table 1")

```

Below is a simple correlational analysis of points, price, year, red, dom, fol, el, lat and lon (figure 1). As one might have expected, the greatest correlation is between the dom and lat, lon and el. The correlation table is consistent with the results in the previous iteration of this project, price has a moderate positive correlation and year has a weak negative correlation with points. Furthermore, we have also displayed a correlation plot comparing wine critics and points, where each name is a binary variable is equal to 1 if the named critic reviewed the wine and 0 otherwise (figure 2). 


```{r correlational analysis}

# corrplot of primary variables 
wine_reviews.df %>%
  select(points, price, year, red, dom, fol, el, lat, lon) %>%
  cor() %>% corrplot.mixed(title = "Figure 1: Corrleation of Explantory Variables and Price",
    mar = c(1,1,1.5,1))

# critic plot 
wine_reviews.df %>% 
  dplyr::select(points, contains("name")) %>% 
  setNames(c("points", str_remove(str_subset(names(wine_reviews.df), "name"), "name"))) %>% 
  cor(wine_reviews.df$points) %>% corrplot(title = "Figure 2: Corrleation of Wine Critics and Price",
    mar = c(1,1,1.5,1), cl.pos='n')

```

### Feature Selection 

We employed a multivariate regression analysis to estimate the effect the variables discussed in the previous section have on points. An exhaustive search of linear models on points was performed using the function `regsubsets()` from the leaps package, keeping the best two subsets of explanatory variables for each size. In order to reduce the computational demands of the performing and exhaustive search on linear models, we have chosen only to use critics Matt Kettmann and Michael Schachner as they have the highest correlation with points. The models were then each evaluated using Mallows’s Cp (Cp), Bayesian Information Criterion (BIC), and the Adjusted R Squared (figures 3 and 4). The model that estimated points using price, year, red, dom, fol, lat, Matt Kettmann, and Michael Schachner minimized the Cp, BIC, and Adjusted R Squared (figures 3 through 8). The variance inflation factor (VIF) of the full model suggested no multicollinearity between coefficients. Furthermore, when trained and tested on the separate partitions of the data the afirmentioned model performed similarly in the training (R2 = 0.20) and test datasets (R2 = 0.18). Table 2 below displays a summary of the linear regression. The results of this model are consistent with the findings in our previous project, both year and price are postive and statistically significant. 

```{r feature selection analysis, echo = F}

# -----------------------------------------------------------------------------
# Exhaustive model search 
# -----------------------------------------------------------------------------

# generate tables 
wine_reviews.df <- wine_reviews.df %>% 
  select(points, price, year, red, dom, fol, el, lat, lon, "nameMatt Kettmann", "nameMichael Schachner")
regsub <- regsubsets(points ~ . , data = wine_reviews.df, method = "exhaustive", nbest = 2)
# summary(regsub)

# Make plots 
plot(regsub, scale = "adjr2", main = "Figure 3: Adjusted R Squared")
plot(regsub, scale = "bic",  main = "Figure 4: BIC")
plot(regsub, scale = "Cp", main = "Figure 5: Cp")

# Make more plots 
p.info <- subsets(regsub, statistic = "adjr2", legend = F, min.size = 3, main = "Figure 6: Adjusted R Squared")
p.info <- subsets(regsub,statistic = "bic", legend = F, min.size = 3, main = "Figure 7: BIC")
p.info <- subsets(regsub,statistic = "cp", legend = F, min.size = 3, main = "Figure 8: Cp")
# abline(a = 1, b = 1, lty = 2)

# Return best model using each of the metrics
# which.max(summary(regsub)$adjr2)
# which.min(summary(regsub)$bic)
# which.min(summary(regsub)$cp)

# Build model
adjr2.m <- lm(data = wine_reviews.df, formula = points ~ .)
# summary(adjr2.m)

# Check for multicollinearity
# vif(adjr2.m)

# -----------------------------------------------------------------------------
# Test full model 
# -----------------------------------------------------------------------------

# Partition dataset
wine_reviews.df <- mutate(wine_reviews.df, id = row_number(points))
train.df <- wine_reviews.df %>% sample_frac(.75)
test.df  <- anti_join(wine_reviews.df, train.df , by = 'id')

# Estimate model
model.m <- lm(points ~ . -id, data = train.df)

# Make predictions and compute the R2, RMSE and MAE
# predict.v <- model.m %>% predict(test.df)
# data.frame(
#   R2 = R2(predict.v, test.df$points),
#   RMSE = RMSE(predict.v, test.df$points),
#   MAE = MAE(predict.v, test.df$points)
# )

# Output regression table 
wine_reviews.df %>% 
  lm(formula = points ~ . -id -el -lon) %>% 
  xtable() %>% 
  kable(
    digits = 5, 
    caption = "Table 2") %>%
  kable_styling(bootstrap_options = "striped", full_width = F,
  position = "left")

```


### Lasso Regression 

Although the stepwise regression revealed meaningful insights about the data, stepwise regression is often criticized for producing R-squared values that are badly biased to be high. Therefore, Data Wine'ing has chosen to supplement it's feature selection analysis with a LASSO regression. Below we estimate a LASSO regression and perform parameter tuning on the data. Figure 9 displays the mean squared error as $\lambda$ increases and figure 10 dislplays the coefficients of the LASSO regression as $\lambda$ increaes. We find the model that minimized the mean squared error has a $\lambda$ of 0.001 and only sets the parameter of Anne.Krebiehl.MW equal to 0. In contrast the model which gives the most regularized model such that error is within one standard error of the minimum has as $\lambda$ of 0.0599 and sets the parameters of Anne.Krebiehl.MW, Carrie.Dykes, Christina.Pickard, Fiona.Adams, Jeff.Jenssen, Joe.Czerwinski, Kerin.O.Keef, Lauren.Buzzeo, Mike.DeSimone, Roger.Voss, Sean.P..Sullivan, lon and el equal to 0. We find that both model preform similar, explaining roughly $24\%$ and $23\%$ of the variation in the data respectively.  

```{r}

# ----------------------------------------------------------------------------------
# prepare data 
# ----------------------------------------------------------------------------------

# prepare data 
wine_reviews.df <- raw_wine_reviews.df %>%
  mutate(
    red = ifelse(color == "red", 1, 0),
    dom = ifelse(country == "US", 1, 0)
  ) %>%
    rename(
    fol = taster_following, name = taster_name, 
    lat = comp_lat, lon = comp_lon, el = comp_el 
  ) %>% 
   bind_cols(as_tibble(model.matrix(~name + 0, . ))) %>% 
  select(points, price, year, red, dom, fol, el, lat, lon, contains("name"),
    -name, -`nameAnne Krebiehl MW`, -name1) %>% 
  mutate_all(as.numeric) %>%
  filter_all(function(x){!is.na(x)})

# ------------------------------------------------------------------------------
# LASSO 
# ------------------------------------------------------------------------------

# center y, X will be standardized in the modelling function
y <- wine_reviews.df %>% select(points) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X <- wine_reviews.df %>% select(-points) %>% as.matrix()

# 10 fold cross validation
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
lasso <- cv.glmnet(X, y, lambda = lambdas_to_try, standardize = TRUE, nfolds = 10)

# plot cross validation 
par(mar = c(4,5,4,2))
plot(lasso, axes = F)
axis(side = 1, at = -10:10)
title("Figure 9: LASSO Parameter Tuning", line = 3)

# -----------------------------------------------------------------------------
# Test 
# -----------------------------------------------------------------------------

# fit model that minimizes square residuals 
lambda <- lasso$lambda.min
model <- glmnet(X, y, lambda = lambda, standardize = TRUE)
y_hat <- predict(model, X)
ssr <- t(y - y_hat) %*% (y - y_hat)
rsq_lasso_min <- cor(y, y_hat)^2
# model$beta

lambda <- lasso$lambda.1se
model <- glmnet(X, y, lambda = lambda, standardize = TRUE)
y_hat <- predict(model, X)
ssr <- t(y - y_hat) %*% (y - y_hat)
rsq_lasso_1se <- cor(y, y_hat)^2
# model$beta

# ---------------------------------------------------------------------------
# plot 
# ---------------------------------------------------------------------------

res <- glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = FALSE)
plot_glmnet(res, xvar = "lambda", label = T, xlim= c(-7, 2), col = brewer.pal(5, "Dark2"), main = "Figure 10: LASSO Regression Coefficients")

# rm(X, y, lambda, y_hat, ssr, model, res)
```





<!-----------------------------------------------------------------------------
Michael  
------------------------------------------------------------------------------> 

### Logistic Regressions 

Recall that we are interested in informing wine consumers what the best predictors are for selecting a quality wine, whether they're buying at their local wine store, or tasting wine at their favorite farmers market, we'd like to determine a metric for what really does constitute a "good" wine. In this section we aim to construct a logistic model that is capable of predicting whether a bottle of wine is above 88 points. 
The output variable of this logistic model will be `p88`, which is equal to 1 if a bottle has a score of above 88 points and 0 otherwise. An 88 point cutoff was selected as it nearly slices the dataset in half and makes for good use in model training and testing, including the fact that this score represents the delineation for what is "very good" and "excellent" wines and is just above the average score for all wine enthusiast reviews (Wine Enthusiast Magazine and Website, 2019). The model `glm(formula = p88 ~ . -points, data = wine_reviews.df, family = binomial(link="logit"))` was run to estimate `p88` and the output can be found in the table below (table 3).

We observe significant coefficients for all predictor variables, with the exception of `lat` and wine reviewers `taster_nameChristina Pickard` and `taster_nameVirginie Boone`. Notably, however, `fol`, `el`, `lat`, and `lon` had a negligible impact on the odds-ratio for `p88` i.e., when points increase by one unit, these variables have no effect on the odds-ratio. This is interesting given the long-studied effects of geography on wine production - one may expect that growing wine in  a place better-suited for growing grapes would produce a wine that is rated more highly than others, but our model suggests location has negligible impact on point value assigned to the wine review. This begs the question of what effect a particular reviewer has on point assignment. As we can see from the results in the model, and the odds-ratio coefficients, the levels in `tasterName` i.e., those who reviewed the wine, play a significant role in predicting if a wine will be scored 89 points or better. More research is required to come to a granular analysis on what exactly may influence the bias an individual reviewer has on a wine. 
The confusion matrix for the model shows that the model was run with 73.5% accuracy (table 4); the ROC-AUC curve accompanying this model returns a value 0.82, suggesting the model performs well when distinguishing between wines scored 89 or better (figure 11).


```{r model eval}

# -----------------------------------------------------------------------------
# read in data 
# -----------------------------------------------------------------------------

# Read in data 
wine_reviews.df <-  raw_wine_reviews.df %>%
  mutate(
    red = as.factor(ifelse(color == "red", 1, 0)),
    dom = as.factor(ifelse(country == "US", 1, 0)),
    p88 = ifelse(points > 88, 1, 0)
  ) %>%
  dplyr::select(
    points, price, year, red, dom, taster_following, comp_el, comp_lat, comp_lon, p88, taster_name
  ) %>% 
  rename(
    fol = taster_following, name = taster_name, 
    lat = comp_lat, lon = comp_lon, el = comp_el 
  ) %>% 
  filter_all(function(x){!is.na(x)})

# -----------------------------------------------------------------------------
# estimate and evaluate model 
# -----------------------------------------------------------------------------

# Estimate model 
glm <- glm(formula = p88 ~ . -points, data = wine_reviews.df, family = binomial(link = "logit"))
# exp(coef(glm)) 
xkabledply(glm, title = "Table 3: Logistic Regression")

# Accuracy 
cm <- regclass::confusion_matrix(glm)
accuracy_glm <- (cm[1,1]+cm[2,2])/(cm[1,1] + cm[1,2] + cm[2,1] + cm[2,2])*100
cm %>% kable(caption = "Table 4: Confusion Matrix of Logistic Regression") %>% kable_styling()

# ROC-AUC
wine_reviews.df$prob <- predict(glm, type=c("response"))
roc <- pROC::roc(p88 ~ prob, data=wine_reviews.df)
plot(roc, main = "Figure 11: ROC-AUC Curve") 

```

```{r train and test}

# Scale dataset 
wine_reviews_z <- uzscale(wine_reviews.df, append=0, excl=c("p88"))

# Sample and subset 
wine_reviews_sample <- sample(2, nrow(wine_reviews_z), replace = TRUE, prob = c(0.75, 0.25))
wine_reviews_train <- filter(wine_reviews_z, wine_reviews_sample == 1) 
wine_reviews_test <- filter(wine_reviews_z, wine_reviews_sample == 2)

# Estimate logistic model using training data 
wine_reviews_logit <- glm(p88 ~ . -points, data = wine_reviews_train, family = binomial)
# summary(wine_reviews_logit)

wine_reviews_logit$xlevels[["taster_name"]] <- union(wine_reviews_logit$xlevels[["taster_name"]], levels(wine_reviews_test$taster_name))

# Use cutoff rule to classify type 
wine_reviews_test <- wine_reviews_test %>%
  mutate(
    p88_logit_p = predict(wine_reviews_logit, wine_reviews_test, type = c("response")),
    p88_logit = ifelse(p88_logit_p > 0.5, 1, 0)
  )

# Calculate accuracy
t <- table(wine_reviews_test$p88, wine_reviews_test$p88_logit)
# (t[1,1] + t[2,2]) / nrow(wine_reviews_test)

# Clean up
rm(wine_reviews_z, wine_reviews_sample, wine_reviews_train, wine_reviews_test, wine_reviews_logit, t)
```

<!-----------------------------------------------------------------------------
Bryan
------------------------------------------------------------------------------> 

### K Nearst Neighbor 

K-Nearest Neighbors (kNN), one of the simplest clustering algorithms, is a supervised, non-parametric, instance-based learning algorithm that utilizes the whole dataset to produce a model. To predict a label for a new input data point, kNN examines the closest or ‘nearest’ neighbors of the input data in the space of feature vectors then outputs the label that appeared most often in that space (Bruce, et al., 2017; Burkov, 2019; Theobald, 2017). Here, nine features from the WineEnthusiast Wine Reviews dataset were chosen for analysis to predict a tenth feature, Points (p88). Of the nine features chosen, taster_name was not numeric and therefore not appropriate for kNN analysis. The final model used eight features, or variables from approximately 24,000 observations, with k-values ranging from k=3 to k=15. Even numbered k-values were avoided to eliminate the possibility of statistical stalemate and an invalid result. The general default for kNN analysis is k=5. Setting the k-value too low may increase bias into the model, however setting the k-value too high, although it may increase model accuracy, also increases computational expense (Theobald, 2017). For this analysis, the best overall model based on accuracy was generated using k=13. Increasing k-value to k=15 resulted in a slightly better recall rate, but lower F1 score, however, the differences were negligible and not worth the increased computational expense. With our dataset, setting k equal to 13 did improve accuracy over lower k-values without slowing the computational efficiency of the model. The model was shown to be approximately 75% accurate when predicting wine from the dataset with a score greater than or equal to 89 points (table 5). This statistically well-balanced model was a considerable improvement over the null model, which had an accuracy percentage of approximately 50%.



```{r knn}


# -----------------------------------------------------------------------------
# Prepare data 
# -----------------------------------------------------------------------------

# wrangle data 
wine2 <- raw_wine_reviews.df %>%
  dplyr::mutate(
    red = ifelse(color == "red", 1, 0),
    dom = ifelse(country == "US", 1, 0),
    p88 = ifelse(points > 88, 1, 0)
  ) %>%
  dplyr::select(
    p88, price, year, red, dom, taster_following, taster_name,
    comp_lon, comp_lat, comp_el, p88
  ) %>%
  rename(
    fol = taster_following, name = taster_name, 
    lat = comp_lat, lon = comp_lon, el = comp_el 
  ) %>% 
  filter_all(function(x) !is.na(x))

# -----------------------------------------------------------------------------
# Create partition 
# -----------------------------------------------------------------------------

wine2 <- wine2 %>% dplyr::select(-p88, everything())
wine2 <- wine2 %>% dplyr::select(-name, everything())
scaled_wine2 <- as.data.frame(scale(wine2[1:8], center = TRUE, scale = TRUE))

wine2_sample <- sample(2, nrow(scaled_wine2), replace=TRUE, prob=c(0.75, 0.25))

# subset 
wine2_training <- scaled_wine2[wine2_sample==1, 1:8]
wine2_test <- scaled_wine2[wine2_sample==2, 1:8]

# labels 
wine2.trainLabels <- wine2[wine2_sample==1, 9]
wine2.testLabels <- wine2[wine2_sample==2, 9]

# -----------------------------------------------------------------------------
# Find best K 
# -----------------------------------------------------------------------------

# create an empty dataframe to store the results from confusion matrices
cmdf = data.frame()

for (kval in 3:15) {
  # predict 
  wine2_pred <- knn(train = wine2_training, test = wine2_test, cl = wine2.trainLabels, k = kval)
  cm <- confusionMatrix(wine2_pred, reference = factor(wine2.testLabels ))
  # construct table 
  accuracy <- cm$overall['Accuracy']
  cmt <- data.frame(K = kval, Accuracy = accuracy, row.names = NULL ) # initialize a row of the metrics 
  cmt <- cbind(cmt, data.frame(t(cm$byClass))) # the dataframe of the transpose, with k valued added in front
  cmdf = rbind(cmdf, cmt)
}

# print table 
cmdf %>% 
  dplyr::select(K, Accuracy, Precision, Recall, Sensitivity, F1) %>% 
  xkabledply(title = "Table 5: Summary Statistics of kNN Classification")

```



<!-----------------------------------------------------------------------------
Reema 
------------------------------------------------------------------------------> 

### K Means 

Clustering techniques are crucial to data mining, especially when working with big data - their ability to create structure in a dataset allows the researcher to draw preliminary insights. To narrow down a dataset to a manageable size, clustering allows one to group observations that are alike. K-means is commonly used for splitting a dataset into a set number of groups. For this analysis using the wine reviews dataset, we are interested in identifying variables that could be strong predictors of the points a reviewer will score the wine. Our analysis indicated that k=5 is the optimal number of groups by which the data could be divided. Upon running a k-means, the model indicated that `fol`is a strong predictor of wine score. For example, twitter users that have around 10,000 twitter followers have almost exclusively reviewed wines after 2000 with an average price of 32 USD. Additionally, through the latitude and longitude graphs we learned that cluster one has only reviewed wines from the coordinates 46.6 N, 119 W which fall on the west coast of the United States. By aggregating all this information, we are able to parse a demographic of the wine reviewers.



```{r kmeans, fig.align = "center"}

# -----------------------------------------------------------------------------
# Prepare data 
# -----------------------------------------------------------------------------

winedata <- raw_wine_reviews.df
wine <- winedata %>% 
 dplyr::mutate(
    red = ifelse(color == "red", 1, 0),
    dom = ifelse(country == "US", 1, 0)
  ) %>%
 dplyr::select(points, price, year, taster_following, comp_el, comp_lat, comp_lon) %>% 
 mutate_all(as.numeric) %>% 
 rename(
    fol = taster_following,
    lat = comp_lat, lon = comp_lon, el = comp_el 
  ) %>% 
 filter_all(function(x){!is.na(x)}) 

# Remove Outliers
outliers <- boxplot(wine$price, plot=FALSE)$out
wine2<- wine[-which(wine$price %in% outliers),]

# -----------------------------------------------------------------------------
# K Means 
#   Analysis inspired by https://uc-r.github.io/kmeans_clustering 
# ---------------------------------------------------------------------------

# function to compute total within-cluster sum of square
wss <- function(k) {
  kmeans(wine2, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
  type = "b", pch = 19, frame = FALSE,
  xlab = "Number of clusters K",
  ylab = "Total within-clusters sum of squares"
)

# n start is just the number of configurations, we dont really need that
k <- kmeans(wine2, centers = 5)
# print(k)

# Plots to compare
fviz_cluster(k, geom = "point",  data = wine2) + ggtitle("k = 5")

# Cluster Mean
# k$centers
# Number of values in each cluster
# k$size

# ---------------------------------------------------------------------------
# Summary statistics of clusters 
# ---------------------------------------------------------------------------

# mean
wine2 %>%
  mutate(Cluster = k$cluster) %>%
  group_by(Cluster) %>%
  summarize_all("mean") %>% 
  kable() %>% kable_styling()

# median
wine2 %>%
  mutate(Cluster = k$cluster) %>%
  group_by(Cluster) %>%
  summarize_all("median") %>% 
  kable() %>% kable_styling()

# ----------------------------------------------------------------------------
# Cluster box plots 
# ----------------------------------------------------------------------------

a <- clusterPlot(wine2, points, "Points")
b <- clusterPlot(wine2, price, "Price")
c <- clusterPlot(wine2, fol, "Taster Following")
d <- clusterPlot(wine2, lon, "Longitude")
e <- clusterPlot(wine2, lat, "Latitude")
f <- clusterPlot(wine2, el, "Elavation")

grid.arrange(a, b, ncol = 2)
grid.arrange(c, d, ncol = 2)
grid.arrange(e, f, ncol = 2)


```


### Results and Discussion 

Our analysis of wine quality using feature selection methods such as LASSO  and stepwise regression were consistent with the findings in the previous iteration of the project (Irish, Longmuir, Domadia, & Pagan, 2020). The parameters price and year we both found to be positive and statically significant. Our analysis also revealed that although a critic’s social media presence does not have an influence over wine quality, there exists biased critics in the sample. 

The results of the logistic regression showed that those who critiqued the wine had an influence on the point value assigned to it and where the wine came from had little influence on how it was scored. This suggests that there may exist biases when a wine is scored by a particular reviewer and, contrary to popular belief, that a wine’s origins is a poor determinant for “good” wine.

Cluster analysis using a k-Nearest Neighbors algorithm was performed to determine whether selected features from the WineEnthusiast Wine Reviews dataset could be used to predict the quality of an unknown wine based on points. A value of k=13 was selected based on overall accuracy (table 5). Our kNN model was shown to be nearly 75% accurate at predicting wines with a quality score of 89 points or better. The accuracy of our model showed marked improvement over the approximately 50% accuracy of the null model.

The K-Means analysis demonstrated that the data could be bucketed into 5 groups meaning a change in any of those sub-characteristics results in a change in points. The ability to bucket data in this capacity makes big data more manageable to derive insights.

### Reference 

Irish, B., Longmuir, R., Domadia, R., & Pagan, M. (2020, March 25). DATS6101 Midterm Project. Google Docs. Retrieved May 3, 2020, from https://docs.google.com/document/d/1R95qXd_yhNe6JE4RdblwReYGausgJa7s-x1nl5WGOzQ/edit#

Theobald, O. (2017). Machine Learning for Absolute Beginners (2nd Ed.). Independently published, Oliver Theobald.

Zack Thoutt. (2017, November 27). Wine Reviews. Retrieved March 25, 2020, from https://www.kaggle.com/zynicide/wine-reviews

(2019, October 31). Wine Enthusiast Magazine and Website. wine-searcher. Retrieved May 3, 2020, from https://www.wine-searcher.com/critics-17-wineenthusiast








